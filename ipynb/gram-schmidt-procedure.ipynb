{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Model with number of inputs (p) such $p > 1$ is called multiple regression. We can represent least squares estimates of multiple regression in terms of estimates of univariate linear model. To understand this, let us assume a univariate $(p > 1)$ linear model - $\\mathbf{Y} = \\mathbf{X}\\beta + \\epsilon$.\n",
    "\n",
    "The least square estimate and residuals are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta = \\dfrac{\\sum_{n=1}^{N} x_i y_i}{\\sum_{n=1}^{N} x_i^2 }\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "r_i = y_i - x_i \\hat \\beta\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In convenient vector notation, we let $\\mathbf{y} = (y_1, ..., y_N)^\\intercal$, $\\mathbf{x} = (x_1, ..., x_N)^\\intercal$ and define:\n",
    "\n",
    "\\begin{equation}\n",
    "\\langle \\mathbf{x}, \\mathbf{y} \\rangle = \\sum_{n=1}^{N} x_i y_i = \\mathbf{x}^\\intercal \\mathbf{y}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we can write the parameters in terms of inner product of x and y. \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta = \\dfrac{\\langle \\mathbf{x}, \\mathbf{y} \\rangle}{\\langle \\mathbf{x}, \\mathbf{x} \\rangle}; \n",
    "\\mathbf{r} = \\mathbf{y} - \\mathbf{x} \\hat \\beta\n",
    "\\end{equation}\n",
    "\n",
    "> The inner product notation generalizes the linear regression to different metric spaces, as well as to probability spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the inputs $\\mathbf{x_1}, \\mathbf{x_2}, ..., \\mathbf{x_p}$ are orthogonal, i.e. $\\langle \\mathbf{x_j}, \\mathbf{x_k} \\rangle = 0$ for all $j \\neq k$, then it is easy to check that the multiple least squares estimates $\\beta_j$ are equal to $\\langle \\mathbf{x_j}, \\mathbf{y} \\rangle / \\langle \\mathbf{x_j}, \\mathbf{x_j} \\rangle$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we have an intercept and a single input $\\mathbf{x}$, we can find that \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta_1 = \\dfrac{\\langle \\mathbf{x} - \\bar x \\mathbb{1}, \\mathbf{y} \\rangle}{\\langle \\mathbf{x} - \\bar x \\mathbb{1}, \\mathbf{x} - \\bar x \\mathbb{1} \\rangle}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\bar x = \\sum_{n=1}^{N} x_i / N$ and $\\mathbb{1} = \\mathbf{x}_0$, the vector of N ones. \n",
    "\n",
    "The steps to generate the regression using this procedure - \n",
    "\n",
    "1. Regress $\\mathbf{x}$ on $\\mathbb{1}$ to produce the residual $\\mathbf{z} = \\mathbf{x} - \\bar x \\mathbb{1}$\n",
    "2. Regress $\\mathbf{y}$ on the residual $\\mathbf{z}$ to give the coefficient $\\hat \\beta_1$\n",
    "\n",
    "where, \"regress $\\mathbf{b}$ on $\\mathbf{a}$\" means a single univariate regression of $\\mathbf{b}$ on $\\mathbf{a}$ with no intercept.\n",
    "\n",
    "This process also generalizes to $p$ points and is called **Gram - Schmidt Process**. It can be understood as a process of *Successive orthogonalization* of the inputs, starting from $\\mathbb{1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram-Schmidt Process\n",
    "\n",
    "**ALGORTHM**\n",
    "\n",
    "> - Initialize $\\mathbf{z_0} = \\mathbf{x_0} = \\mathbb{1}$.\n",
    "\n",
    "> - For all $\\mathbf{x_j}$ s.t. $j$ in $\\{1, 2, 3, ..., p\\}$ for $p$ inputs, regress $\\mathbf{x_j}$ on the residuals after $j_th$ step, where the coefficients $\\hat \\gamma_{lj}$ are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\gamma_{lj} = \\dfrac{\\langle \\mathbf{z_l}, \\mathbf{x_j} \\rangle}{\\langle \\mathbf{z_l}, \\mathbf{z_l} \\rangle}\n",
    "\\end{equation}\n",
    "\n",
    "> and residuals at each step are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z_j} = \\mathbf{x_j} - \\sum_{k=0}^{j-1} \\hat \\gamma_{kj}\\mathbf{z_k}\n",
    "\\end{equation}\n",
    "\n",
    "> - Finally, we can calculate $\\hat \\beta_p$ as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta_p = \\dfrac{\\langle \\mathbf{z_p}, \\mathbf{y} \\rangle}{\\langle \\mathbf{z_p}, \\mathbf{z_p} \\rangle}\n",
    "\\end{equation}\n",
    "\n",
    "Let us test this procedure with $p = 2$.\n",
    "\n",
    "As a first step, we will run a Multiple Regression over a set of inputs and get the regression coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [[ 1.39782326  1.83576285 -0.04935882]]\n"
     ]
    }
   ],
   "source": [
    "x1 = np.array([2, 2.2, 3.2, 4.5, 5.0])\n",
    "x2 = np.array([45.0, 20.0, 30.0, 10.0, 6.5])\n",
    "x0 = np.ones(len(x1))\n",
    "z0 = x0\n",
    "y = np.array([2.3, 4.5, 6.7, 8.9, 10.11])\n",
    "\n",
    "X = np.matrix([x0, x1, x2]).T\n",
    "Y = np.matrix(y).T\n",
    "\n",
    "lin_reg = LinearRegression(copy_X=True, fit_intercept=False, n_jobs=1, normalize=False)\n",
    "lin_reg.fit(X, Y)\n",
    "coeffs = lin_reg.coef_\n",
    "print(\"Coefficients: {}\".format(coeffs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us calculate the $\\beta_2$ using the iterative procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma_01 = z0.dot(x1) / (z0.dot(z0))\n",
    "z1 = x1 - gamma_01 * z0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma_02 = z0.dot(x2) / (z0.dot(z0))\n",
    "gamma_12 = z1.dot(x2) / (z1.dot(z1))\n",
    "z2 = x2 - gamma_02 * z0 - gamma_12 * z1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.0493588203647\n"
     ]
    }
   ],
   "source": [
    "beta_p = z2.dot(y) / (z2.dot(z2))\n",
    "print(beta_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can calculate the $\\beta_1$ using the iterative procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.83576285118\n"
     ]
    }
   ],
   "source": [
    "gamma_01 = z0.dot(x2) / (z0.dot(z0))\n",
    "z1 = x2 - gamma_01 * z0\n",
    "\n",
    "gamma_02 = z0.dot(x1) / (z0.dot(z0))\n",
    "gamma_12 = z1.dot(x1) / (z1.dot(z1))\n",
    "z2 = x1 - gamma_02 * z0 - gamma_12 * z1\n",
    "\n",
    "beta_p = z2.dot(y) / (z2.dot(z2))\n",
    "print(beta_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that both $\\beta_1$ and $\\beta_2$ match the regression coefficients obtained via L2 - norm minimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the transformations on $\\mathbf{Z}$ more generally as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{Z} \\mathbf{\\Gamma}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathbf{z_j}$ are the columns of $\\mathbf{Z}$ and $\\mathbf{Gamma}$ is an upper trangular matrix with the coefficients $\\gamma_{lj}$. \n",
    "\n",
    "For $p=2$ case, we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{\\Gamma} =\n",
    "  \\begin{bmatrix}\n",
    "    1 & \\hat \\gamma_{01} & \\hat \\gamma_{02} \\\\\n",
    "    0 & 1 & \\hat \\gamma_{12} \\\\\n",
    "    0 & 0 & 1\n",
    "  \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "This is similar to **QR Decomposition**. We can do a scaled QR-decomposition as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{Z} \\mathbf{D^{-1}} \\mathbf{D} \\mathbf{\\Gamma} = \\mathbf{Q} \\mathbf{R}\n",
    "\\end{equation}\n",
    "\n",
    "where,\n",
    "\n",
    "$D_{jj} = ||\\mathbf{z_j}||$, and\n",
    "$\\mathbf{Q^\\intercal} = \\mathbf{Q^{-1}}$\n",
    "\n",
    "From here, we can calculate $\\hat \\beta$:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat \\beta = (\\mathbf{X^\\intercal} \\mathbf{X})^{-1} \\mathbf{X^\\intercal} y \\\\\n",
    "    = (\\mathbf{(QR)^\\intercal} \\mathbf{QR})^{-1} \\mathbf{(QR)^\\intercal} y \\\\\n",
    "    = (\\mathbf{R^\\intercal} \\mathbf{Q^\\intercal} \\mathbf{Q} \\mathbf{R})^{-1} \\mathbf{R^\\intercal} \\mathbf{Q^\\intercal} y \\\\\n",
    "    = (\\mathbf{R^\\intercal} \\mathbf{I} \\mathbf{R})^{-1} \\mathbf{R^\\intercal} \\mathbf{Q^\\intercal} y \\\\\n",
    "    = (\\mathbf{R^\\intercal} \\mathbf{R})^{-1} \\mathbf{R^\\intercal} \\mathbf{Q^\\intercal} y \\\\\n",
    "    = \\mathbf{R^{-1}} (\\mathbf{R^\\intercal})^{-1} \\mathbf{R^\\intercal} \\mathbf{Q^\\intercal} y \\\\\n",
    "    = \\mathbf{R^{-1}}  \\mathbf{Q^\\intercal} y \\\\\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat {\\mathbf{y}} = \\mathbf{X} \\hat \\beta \\\\\n",
    "    = \\mathbf{Q} \\mathbf{R} \\mathbf{R^{-1}} \\mathbf{Q^\\intercal} y \\\\\n",
    "    = \\mathbf{Q} \\mathbf{Q^\\intercal} y \n",
    "\\end{equation}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
