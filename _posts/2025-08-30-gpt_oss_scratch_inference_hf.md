---
title: GPT OSS from Scratch -- Inference Huggingface Model
description: >-
  Testing out the huggingface version for the gpt-oss-20b locally on an RTX 4090
date: 2025-08-20
categories: [Blog]
tags: [AI, Machine Learning, GPT]
pin: true
math: false
author: ks
---

![GPT OSS 20B](/assets/gpt_oss_20b.jpg)


```python
from transformers import AutoModel
from transformers import pipeline

# Either do this or below -- othewise model will OOM
model_id = "openai/gpt-oss-20b"
model = AutoModel.from_pretrained(model_id, device_map="cuda")
print(model)
parameters_state_dict = model.state_dict()
for key, value in parameters_state_dict.items():
    print(key, value.size())
    
```


```text
GptOssModel(
  (embed_tokens): Embedding(201088, 2880, padding_idx=199999)
  (layers): ModuleList(
    (0-23): 24 x GptOssDecoderLayer(
      (self_attn): GptOssAttention(
        (q_proj): Linear(in_features=2880, out_features=4096, bias=True)
        (k_proj): Linear(in_features=2880, out_features=512, bias=True)
        (v_proj): Linear(in_features=2880, out_features=512, bias=True)
        (o_proj): Linear(in_features=4096, out_features=2880, bias=True)
      )
      (mlp): GptOssMLP(
        (router): GptOssTopKRouter()
        (experts): Mxfp4GptOssExperts()
      )
      (input_layernorm): GptOssRMSNorm((2880,), eps=1e-05)
      (post_attention_layernorm): GptOssRMSNorm((2880,), eps=1e-05)
    )
  )
  (norm): GptOssRMSNorm((2880,), eps=1e-05)
  (rotary_emb): GptOssRotaryEmbedding()
)
embed_tokens.weight torch.Size([201088, 2880])
layers.0.self_attn.sinks torch.Size([64])
layers.0.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.0.self_attn.q_proj.bias torch.Size([4096])
layers.0.self_attn.k_proj.weight torch.Size([512, 2880])
layers.0.self_attn.k_proj.bias torch.Size([512])
layers.0.self_attn.v_proj.weight torch.Size([512, 2880])
layers.0.self_attn.v_proj.bias torch.Size([512])
layers.0.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.0.self_attn.o_proj.bias torch.Size([2880])
layers.0.mlp.router.weight torch.Size([32, 2880])
layers.0.mlp.router.bias torch.Size([32])
layers.0.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.0.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.0.input_layernorm.weight torch.Size([2880])
layers.0.post_attention_layernorm.weight torch.Size([2880])
layers.1.self_attn.sinks torch.Size([64])
layers.1.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.1.self_attn.q_proj.bias torch.Size([4096])
layers.1.self_attn.k_proj.weight torch.Size([512, 2880])
layers.1.self_attn.k_proj.bias torch.Size([512])
layers.1.self_attn.v_proj.weight torch.Size([512, 2880])
layers.1.self_attn.v_proj.bias torch.Size([512])
layers.1.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.1.self_attn.o_proj.bias torch.Size([2880])
layers.1.mlp.router.weight torch.Size([32, 2880])
layers.1.mlp.router.bias torch.Size([32])
layers.1.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.1.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.1.input_layernorm.weight torch.Size([2880])
layers.1.post_attention_layernorm.weight torch.Size([2880])
layers.2.self_attn.sinks torch.Size([64])
layers.2.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.2.self_attn.q_proj.bias torch.Size([4096])
layers.2.self_attn.k_proj.weight torch.Size([512, 2880])
layers.2.self_attn.k_proj.bias torch.Size([512])
layers.2.self_attn.v_proj.weight torch.Size([512, 2880])
layers.2.self_attn.v_proj.bias torch.Size([512])
layers.2.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.2.self_attn.o_proj.bias torch.Size([2880])
layers.2.mlp.router.weight torch.Size([32, 2880])
layers.2.mlp.router.bias torch.Size([32])
layers.2.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.2.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.2.input_layernorm.weight torch.Size([2880])
layers.2.post_attention_layernorm.weight torch.Size([2880])
layers.3.self_attn.sinks torch.Size([64])
layers.3.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.3.self_attn.q_proj.bias torch.Size([4096])
layers.3.self_attn.k_proj.weight torch.Size([512, 2880])
layers.3.self_attn.k_proj.bias torch.Size([512])
layers.3.self_attn.v_proj.weight torch.Size([512, 2880])
layers.3.self_attn.v_proj.bias torch.Size([512])
layers.3.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.3.self_attn.o_proj.bias torch.Size([2880])
layers.3.mlp.router.weight torch.Size([32, 2880])
layers.3.mlp.router.bias torch.Size([32])
layers.3.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.3.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.3.input_layernorm.weight torch.Size([2880])
layers.3.post_attention_layernorm.weight torch.Size([2880])
layers.4.self_attn.sinks torch.Size([64])
layers.4.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.4.self_attn.q_proj.bias torch.Size([4096])
layers.4.self_attn.k_proj.weight torch.Size([512, 2880])
layers.4.self_attn.k_proj.bias torch.Size([512])
layers.4.self_attn.v_proj.weight torch.Size([512, 2880])
layers.4.self_attn.v_proj.bias torch.Size([512])
layers.4.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.4.self_attn.o_proj.bias torch.Size([2880])
layers.4.mlp.router.weight torch.Size([32, 2880])
layers.4.mlp.router.bias torch.Size([32])
layers.4.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.4.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.4.input_layernorm.weight torch.Size([2880])
layers.4.post_attention_layernorm.weight torch.Size([2880])
layers.5.self_attn.sinks torch.Size([64])
layers.5.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.5.self_attn.q_proj.bias torch.Size([4096])
layers.5.self_attn.k_proj.weight torch.Size([512, 2880])
layers.5.self_attn.k_proj.bias torch.Size([512])
layers.5.self_attn.v_proj.weight torch.Size([512, 2880])
layers.5.self_attn.v_proj.bias torch.Size([512])
layers.5.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.5.self_attn.o_proj.bias torch.Size([2880])
layers.5.mlp.router.weight torch.Size([32, 2880])
layers.5.mlp.router.bias torch.Size([32])
layers.5.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.5.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.5.input_layernorm.weight torch.Size([2880])
layers.5.post_attention_layernorm.weight torch.Size([2880])
layers.6.self_attn.sinks torch.Size([64])
layers.6.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.6.self_attn.q_proj.bias torch.Size([4096])
layers.6.self_attn.k_proj.weight torch.Size([512, 2880])
layers.6.self_attn.k_proj.bias torch.Size([512])
layers.6.self_attn.v_proj.weight torch.Size([512, 2880])
layers.6.self_attn.v_proj.bias torch.Size([512])
layers.6.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.6.self_attn.o_proj.bias torch.Size([2880])
layers.6.mlp.router.weight torch.Size([32, 2880])
layers.6.mlp.router.bias torch.Size([32])
layers.6.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.6.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.6.input_layernorm.weight torch.Size([2880])
layers.6.post_attention_layernorm.weight torch.Size([2880])
layers.7.self_attn.sinks torch.Size([64])
layers.7.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.7.self_attn.q_proj.bias torch.Size([4096])
layers.7.self_attn.k_proj.weight torch.Size([512, 2880])
layers.7.self_attn.k_proj.bias torch.Size([512])
layers.7.self_attn.v_proj.weight torch.Size([512, 2880])
layers.7.self_attn.v_proj.bias torch.Size([512])
layers.7.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.7.self_attn.o_proj.bias torch.Size([2880])
layers.7.mlp.router.weight torch.Size([32, 2880])
layers.7.mlp.router.bias torch.Size([32])
layers.7.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.7.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.7.input_layernorm.weight torch.Size([2880])
layers.7.post_attention_layernorm.weight torch.Size([2880])
layers.8.self_attn.sinks torch.Size([64])
layers.8.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.8.self_attn.q_proj.bias torch.Size([4096])
layers.8.self_attn.k_proj.weight torch.Size([512, 2880])
layers.8.self_attn.k_proj.bias torch.Size([512])
layers.8.self_attn.v_proj.weight torch.Size([512, 2880])
layers.8.self_attn.v_proj.bias torch.Size([512])
layers.8.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.8.self_attn.o_proj.bias torch.Size([2880])
layers.8.mlp.router.weight torch.Size([32, 2880])
layers.8.mlp.router.bias torch.Size([32])
layers.8.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.8.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.8.input_layernorm.weight torch.Size([2880])
layers.8.post_attention_layernorm.weight torch.Size([2880])
layers.9.self_attn.sinks torch.Size([64])
layers.9.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.9.self_attn.q_proj.bias torch.Size([4096])
layers.9.self_attn.k_proj.weight torch.Size([512, 2880])
layers.9.self_attn.k_proj.bias torch.Size([512])
layers.9.self_attn.v_proj.weight torch.Size([512, 2880])
layers.9.self_attn.v_proj.bias torch.Size([512])
layers.9.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.9.self_attn.o_proj.bias torch.Size([2880])
layers.9.mlp.router.weight torch.Size([32, 2880])
layers.9.mlp.router.bias torch.Size([32])
layers.9.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.9.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.9.input_layernorm.weight torch.Size([2880])
layers.9.post_attention_layernorm.weight torch.Size([2880])
layers.10.self_attn.sinks torch.Size([64])
layers.10.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.10.self_attn.q_proj.bias torch.Size([4096])
layers.10.self_attn.k_proj.weight torch.Size([512, 2880])
layers.10.self_attn.k_proj.bias torch.Size([512])
layers.10.self_attn.v_proj.weight torch.Size([512, 2880])
layers.10.self_attn.v_proj.bias torch.Size([512])
layers.10.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.10.self_attn.o_proj.bias torch.Size([2880])
layers.10.mlp.router.weight torch.Size([32, 2880])
layers.10.mlp.router.bias torch.Size([32])
layers.10.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.10.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.10.input_layernorm.weight torch.Size([2880])
layers.10.post_attention_layernorm.weight torch.Size([2880])
layers.11.self_attn.sinks torch.Size([64])
layers.11.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.11.self_attn.q_proj.bias torch.Size([4096])
layers.11.self_attn.k_proj.weight torch.Size([512, 2880])
layers.11.self_attn.k_proj.bias torch.Size([512])
layers.11.self_attn.v_proj.weight torch.Size([512, 2880])
layers.11.self_attn.v_proj.bias torch.Size([512])
layers.11.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.11.self_attn.o_proj.bias torch.Size([2880])
layers.11.mlp.router.weight torch.Size([32, 2880])
layers.11.mlp.router.bias torch.Size([32])
layers.11.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.11.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.11.input_layernorm.weight torch.Size([2880])
layers.11.post_attention_layernorm.weight torch.Size([2880])
layers.12.self_attn.sinks torch.Size([64])
layers.12.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.12.self_attn.q_proj.bias torch.Size([4096])
layers.12.self_attn.k_proj.weight torch.Size([512, 2880])
layers.12.self_attn.k_proj.bias torch.Size([512])
layers.12.self_attn.v_proj.weight torch.Size([512, 2880])
layers.12.self_attn.v_proj.bias torch.Size([512])
layers.12.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.12.self_attn.o_proj.bias torch.Size([2880])
layers.12.mlp.router.weight torch.Size([32, 2880])
layers.12.mlp.router.bias torch.Size([32])
layers.12.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.12.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.12.input_layernorm.weight torch.Size([2880])
layers.12.post_attention_layernorm.weight torch.Size([2880])
layers.13.self_attn.sinks torch.Size([64])
layers.13.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.13.self_attn.q_proj.bias torch.Size([4096])
layers.13.self_attn.k_proj.weight torch.Size([512, 2880])
layers.13.self_attn.k_proj.bias torch.Size([512])
layers.13.self_attn.v_proj.weight torch.Size([512, 2880])
layers.13.self_attn.v_proj.bias torch.Size([512])
layers.13.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.13.self_attn.o_proj.bias torch.Size([2880])
layers.13.mlp.router.weight torch.Size([32, 2880])
layers.13.mlp.router.bias torch.Size([32])
layers.13.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.13.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.13.input_layernorm.weight torch.Size([2880])
layers.13.post_attention_layernorm.weight torch.Size([2880])
layers.14.self_attn.sinks torch.Size([64])
layers.14.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.14.self_attn.q_proj.bias torch.Size([4096])
layers.14.self_attn.k_proj.weight torch.Size([512, 2880])
layers.14.self_attn.k_proj.bias torch.Size([512])
layers.14.self_attn.v_proj.weight torch.Size([512, 2880])
layers.14.self_attn.v_proj.bias torch.Size([512])
layers.14.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.14.self_attn.o_proj.bias torch.Size([2880])
layers.14.mlp.router.weight torch.Size([32, 2880])
layers.14.mlp.router.bias torch.Size([32])
layers.14.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.14.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.14.input_layernorm.weight torch.Size([2880])
layers.14.post_attention_layernorm.weight torch.Size([2880])
layers.15.self_attn.sinks torch.Size([64])
layers.15.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.15.self_attn.q_proj.bias torch.Size([4096])
layers.15.self_attn.k_proj.weight torch.Size([512, 2880])
layers.15.self_attn.k_proj.bias torch.Size([512])
layers.15.self_attn.v_proj.weight torch.Size([512, 2880])
layers.15.self_attn.v_proj.bias torch.Size([512])
layers.15.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.15.self_attn.o_proj.bias torch.Size([2880])
layers.15.mlp.router.weight torch.Size([32, 2880])
layers.15.mlp.router.bias torch.Size([32])
layers.15.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.15.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.15.input_layernorm.weight torch.Size([2880])
layers.15.post_attention_layernorm.weight torch.Size([2880])
layers.16.self_attn.sinks torch.Size([64])
layers.16.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.16.self_attn.q_proj.bias torch.Size([4096])
layers.16.self_attn.k_proj.weight torch.Size([512, 2880])
layers.16.self_attn.k_proj.bias torch.Size([512])
layers.16.self_attn.v_proj.weight torch.Size([512, 2880])
layers.16.self_attn.v_proj.bias torch.Size([512])
layers.16.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.16.self_attn.o_proj.bias torch.Size([2880])
layers.16.mlp.router.weight torch.Size([32, 2880])
layers.16.mlp.router.bias torch.Size([32])
layers.16.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.16.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.16.input_layernorm.weight torch.Size([2880])
layers.16.post_attention_layernorm.weight torch.Size([2880])
layers.17.self_attn.sinks torch.Size([64])
layers.17.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.17.self_attn.q_proj.bias torch.Size([4096])
layers.17.self_attn.k_proj.weight torch.Size([512, 2880])
layers.17.self_attn.k_proj.bias torch.Size([512])
layers.17.self_attn.v_proj.weight torch.Size([512, 2880])
layers.17.self_attn.v_proj.bias torch.Size([512])
layers.17.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.17.self_attn.o_proj.bias torch.Size([2880])
layers.17.mlp.router.weight torch.Size([32, 2880])
layers.17.mlp.router.bias torch.Size([32])
layers.17.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.17.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.17.input_layernorm.weight torch.Size([2880])
layers.17.post_attention_layernorm.weight torch.Size([2880])
layers.18.self_attn.sinks torch.Size([64])
layers.18.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.18.self_attn.q_proj.bias torch.Size([4096])
layers.18.self_attn.k_proj.weight torch.Size([512, 2880])
layers.18.self_attn.k_proj.bias torch.Size([512])
layers.18.self_attn.v_proj.weight torch.Size([512, 2880])
layers.18.self_attn.v_proj.bias torch.Size([512])
layers.18.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.18.self_attn.o_proj.bias torch.Size([2880])
layers.18.mlp.router.weight torch.Size([32, 2880])
layers.18.mlp.router.bias torch.Size([32])
layers.18.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.18.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.18.input_layernorm.weight torch.Size([2880])
layers.18.post_attention_layernorm.weight torch.Size([2880])
layers.19.self_attn.sinks torch.Size([64])
layers.19.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.19.self_attn.q_proj.bias torch.Size([4096])
layers.19.self_attn.k_proj.weight torch.Size([512, 2880])
layers.19.self_attn.k_proj.bias torch.Size([512])
layers.19.self_attn.v_proj.weight torch.Size([512, 2880])
layers.19.self_attn.v_proj.bias torch.Size([512])
layers.19.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.19.self_attn.o_proj.bias torch.Size([2880])
layers.19.mlp.router.weight torch.Size([32, 2880])
layers.19.mlp.router.bias torch.Size([32])
layers.19.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.19.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.19.input_layernorm.weight torch.Size([2880])
layers.19.post_attention_layernorm.weight torch.Size([2880])
layers.20.self_attn.sinks torch.Size([64])
layers.20.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.20.self_attn.q_proj.bias torch.Size([4096])
layers.20.self_attn.k_proj.weight torch.Size([512, 2880])
layers.20.self_attn.k_proj.bias torch.Size([512])
layers.20.self_attn.v_proj.weight torch.Size([512, 2880])
layers.20.self_attn.v_proj.bias torch.Size([512])
layers.20.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.20.self_attn.o_proj.bias torch.Size([2880])
layers.20.mlp.router.weight torch.Size([32, 2880])
layers.20.mlp.router.bias torch.Size([32])
layers.20.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.20.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.20.input_layernorm.weight torch.Size([2880])
layers.20.post_attention_layernorm.weight torch.Size([2880])
layers.21.self_attn.sinks torch.Size([64])
layers.21.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.21.self_attn.q_proj.bias torch.Size([4096])
layers.21.self_attn.k_proj.weight torch.Size([512, 2880])
layers.21.self_attn.k_proj.bias torch.Size([512])
layers.21.self_attn.v_proj.weight torch.Size([512, 2880])
layers.21.self_attn.v_proj.bias torch.Size([512])
layers.21.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.21.self_attn.o_proj.bias torch.Size([2880])
layers.21.mlp.router.weight torch.Size([32, 2880])
layers.21.mlp.router.bias torch.Size([32])
layers.21.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.21.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.21.input_layernorm.weight torch.Size([2880])
layers.21.post_attention_layernorm.weight torch.Size([2880])
layers.22.self_attn.sinks torch.Size([64])
layers.22.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.22.self_attn.q_proj.bias torch.Size([4096])
layers.22.self_attn.k_proj.weight torch.Size([512, 2880])
layers.22.self_attn.k_proj.bias torch.Size([512])
layers.22.self_attn.v_proj.weight torch.Size([512, 2880])
layers.22.self_attn.v_proj.bias torch.Size([512])
layers.22.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.22.self_attn.o_proj.bias torch.Size([2880])
layers.22.mlp.router.weight torch.Size([32, 2880])
layers.22.mlp.router.bias torch.Size([32])
layers.22.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.22.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.22.input_layernorm.weight torch.Size([2880])
layers.22.post_attention_layernorm.weight torch.Size([2880])
layers.23.self_attn.sinks torch.Size([64])
layers.23.self_attn.q_proj.weight torch.Size([4096, 2880])
layers.23.self_attn.q_proj.bias torch.Size([4096])
layers.23.self_attn.k_proj.weight torch.Size([512, 2880])
layers.23.self_attn.k_proj.bias torch.Size([512])
layers.23.self_attn.v_proj.weight torch.Size([512, 2880])
layers.23.self_attn.v_proj.bias torch.Size([512])
layers.23.self_attn.o_proj.weight torch.Size([2880, 4096])
layers.23.self_attn.o_proj.bias torch.Size([2880])
layers.23.mlp.router.weight torch.Size([32, 2880])
layers.23.mlp.router.bias torch.Size([32])
layers.23.mlp.experts.gate_up_proj_bias torch.Size([32, 5760])
layers.23.mlp.experts.down_proj_bias torch.Size([32, 2880])
layers.23.input_layernorm.weight torch.Size([2880])
layers.23.post_attention_layernorm.weight torch.Size([2880])
norm.weight torch.Size([2880])
```